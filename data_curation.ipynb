{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/alireza/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume word difficulty is directly related to word frequency. Word frequency is obtained from words.db (https://github.com/harshnative/words-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_data():\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(\"corpus/data/words.db\")\n",
    "\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    for i in range(3, 35):\n",
    "        query = f'SELECT * FROM \"{i}\"'\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df.drop(\"ID_I\", axis=1, inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.columns = [\"word\", \"frequency\"]\n",
    "\n",
    "    # convert frequency to int\n",
    "    final_df[\"frequency\"] = final_df[\"frequency\"].astype(int)\n",
    "\n",
    "    # Sort the DataFrame by frequency in descending order\n",
    "    final_df = final_df.sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size is (608943, 2)\n",
      "           word    frequency\n",
      "0           the  23135936835\n",
      "1           and  12997704754\n",
      "2           for   5933354779\n",
      "3          that   3400041846\n",
      "4          this   3228476270\n",
      "...         ...          ...\n",
      "608938  unsteek            1\n",
      "608939  unsteck            1\n",
      "608940  unstate            1\n",
      "608941  unstain            1\n",
      "608942  unstaid            1\n",
      "\n",
      "[608943 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "all_words = read_clean_data()\n",
    "\n",
    "# print df size\n",
    "print(f\"df size is {all_words.shape}\")\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of English words for fast lookup\n",
    "english_words_set = set(words.words())\n",
    "\n",
    "# Identifying English words\n",
    "all_words[\"is_english\"] = np.vectorize(lambda x: x.lower() in english_words_set)(\n",
    "    all_words[\"word\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection, it's clear that the set includes some obscure words. In order to remove them, we join with a set of most common English words (https://github.com/dolph/dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  popular\n",
      "0            aa     True\n",
      "1      aardvark     True\n",
      "2         aargh     True\n",
      "3         aback     True\n",
      "4        abacus     True\n",
      "...         ...      ...\n",
      "25317    zoning     True\n",
      "25318    zonked     True\n",
      "25319       zoo     True\n",
      "25320      zoom     True\n",
      "25321   zooming     True\n",
      "\n",
      "[25322 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# list of all popular words\n",
    "popular = pd.read_csv(\"corpus/data/popular.txt\", header=None, names=[\"word\"])\n",
    "\n",
    "popular[\"popular\"] = True\n",
    "\n",
    "print(popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word   rank  popular\n",
      "0                the      0     True\n",
      "1                and      1     True\n",
      "2                for      2     True\n",
      "3               that      3     True\n",
      "4               this      4     True\n",
      "...              ...    ...      ...\n",
      "49995  refractometry  49995    False\n",
      "49996      reconvert  49996    False\n",
      "49997   planetesimal  49997    False\n",
      "49998  paravertebral  49998    False\n",
      "49999          alula  49999    False\n",
      "\n",
      "[50000 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1170496/613308710.py:12: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"popular\"] = df[\"popular\"].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "df = all_words[(all_words[\"is_english\"])][:50_000].reset_index(drop=True)\n",
    "\n",
    "# drop is_english and frequency\n",
    "df.drop([\"is_english\", \"frequency\"], axis=1, inplace=True)\n",
    "\n",
    "# add index column as a new column called rank\n",
    "df[\"rank\"] = df.index\n",
    "\n",
    "# merge popular words with df\n",
    "df = pd.merge(df, popular, how=\"left\", on=\"word\")\n",
    "# fill NaN with False\n",
    "df[\"popular\"] = df[\"popular\"].fillna(False).astype(bool)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRE/TOEFL/IELTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enrich the vocabulary, we add words from GRE, TOEFL, and IELTS exams obtained from:\n",
    "\n",
    "https://github.com/surajk95/wordsta/tree/master/app/lists\n",
    "\n",
    "https://github.com/lzrk/nglsh/\n",
    "\n",
    "https://github.com/ladrift/toefl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRE words\n",
    "js_files = ['warm-up.js', 'intermediate.js', 'hard.js', 'high-frequency-gre.js']\n",
    "\n",
    "all_js_dfs = []\n",
    "for js_file in js_files:\n",
    "    js_df = pd.read_json(f\"corpus/data/{js_file}\")\n",
    "    all_js_dfs.append(js_df)\n",
    "\n",
    "words_1 = pd.concat(all_js_dfs, ignore_index=True)\n",
    "words_1 = words_1[\"word\"].unique()\n",
    "\n",
    "\n",
    "len(words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(file_path, delimiter):\n",
    "    words = set()  # Use a set to store unique words\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if delimiter in line:  # Only process lines containing the delimiter\n",
    "                word = line.split(delimiter)[0].strip()\n",
    "                words.add(word)  # Add the word to the set (ensures uniqueness)\n",
    "    return list(words)  # Convert the set back to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"corpus/data/IELTS-4000.txt\"\n",
    "delimiter = \":\"\n",
    "words_2 = extract_words(file_path, delimiter)\n",
    "len(words_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"corpus/data/wangyumei-toefl-words.txt\"\n",
    "delimiter = \"#\"\n",
    "words_3 = extract_words(file_path, delimiter)\n",
    "len(words_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               word  exam\n",
      "0            racket  True\n",
      "1           ferment  True\n",
      "2       pack animal  True\n",
      "3      respectively  True\n",
      "4          defecate  True\n",
      "...             ...   ...\n",
      "7280         myriad  True\n",
      "7281        whittle  True\n",
      "7282           dime  True\n",
      "7283           edge  True\n",
      "7284  collaboration  True\n",
      "\n",
      "[7285 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# combine all words using set\n",
    "\n",
    "words_1 = set(words_1)\n",
    "words_2 = set(words_2)\n",
    "words_3 = set(words_3)\n",
    "\n",
    "combined_words = list(words_1 | words_2 | words_3)\n",
    "\n",
    "# convert combined_words to dataframe\n",
    "exam_words = pd.DataFrame({\"word\": combined_words})\n",
    "exam_words['exam'] = True\n",
    "\n",
    "print(exam_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word   rank  popular   exam\n",
      "0                the      0     True  False\n",
      "1                and      1     True  False\n",
      "2                for      2     True  False\n",
      "3               that      3     True  False\n",
      "4               this      4     True  False\n",
      "...              ...    ...      ...    ...\n",
      "49995  refractometry  49995    False  False\n",
      "49996      reconvert  49996    False  False\n",
      "49997   planetesimal  49997    False  False\n",
      "49998  paravertebral  49998    False  False\n",
      "49999          alula  49999    False  False\n",
      "\n",
      "[50000 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1170496/1845559896.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"exam\"] = df[\"exam\"].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# merge exam_words with df\n",
    "df = pd.merge(df, exam_words, how=\"left\", on=\"word\")\n",
    "# fill NaN with False\n",
    "df[\"exam\"] = df[\"exam\"].fillna(False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter for only popular or exam words, keeping their ranks. We will later use this ranks to represent word difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word   rank\n",
      "0           the      0\n",
      "1           and      1\n",
      "2           for      2\n",
      "3          that      3\n",
      "4          this      4\n",
      "...         ...    ...\n",
      "18279  bigamist  49510\n",
      "18280   seclude  49613\n",
      "18281   wangler  49701\n",
      "18282  shipload  49785\n",
      "18283  intubate  49820\n",
      "\n",
      "[18284 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"exam\"] | df[\"popular\"]].drop([\"exam\", \"popular\"], axis=1).reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write column word to csv without header\n",
    "df.to_csv(\"corpus/english_words.csv\", index=False, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
