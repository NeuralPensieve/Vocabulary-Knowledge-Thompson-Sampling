{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/alireza/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_data():\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(\"words.db\")\n",
    "\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    for i in range(3, 35):\n",
    "        query = f'SELECT * FROM \"{i}\"'\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df.drop(\"ID_I\", axis=1, inplace=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.columns = [\"word\", \"frequency\"]\n",
    "\n",
    "    # convert frequency to int\n",
    "    final_df[\"frequency\"] = final_df[\"frequency\"].astype(int)\n",
    "\n",
    "    # Sort the DataFrame by frequency in descending order\n",
    "    final_df = final_df.sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size is (608943, 2)\n",
      "           word    frequency\n",
      "0           the  23135936835\n",
      "1           and  12997704754\n",
      "2           for   5933354779\n",
      "3          that   3400041846\n",
      "4          this   3228476270\n",
      "...         ...          ...\n",
      "608938  unsteek            1\n",
      "608939  unsteck            1\n",
      "608940  unstate            1\n",
      "608941  unstain            1\n",
      "608942  unstaid            1\n",
      "\n",
      "[608943 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "all_words = read_clean_data()\n",
    "\n",
    "# print df size\n",
    "print(f\"df size is {all_words.shape}\")\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of English words for fast lookup\n",
    "english_words_set = set(words.words())\n",
    "\n",
    "# Vectorized approach to check if words are in the English word list\n",
    "all_words[\"is_english\"] = np.vectorize(lambda x: x.lower() in english_words_set)(\n",
    "    all_words[\"word\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                word    frequency  is_english\n",
      "0                the  23135936835        True\n",
      "1                and  12997704754        True\n",
      "2                for   5933354779        True\n",
      "3               that   3400041846        True\n",
      "4               this   3228476270        True\n",
      "...              ...          ...         ...\n",
      "49995  refractometry        13607        True\n",
      "49996      reconvert        13606        True\n",
      "49997   planetesimal        13606        True\n",
      "49998  paravertebral        13604        True\n",
      "49999          alula        13603        True\n",
      "\n",
      "[50000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = all_words[(all_words[\"is_english\"] == True)][:50_000].reset_index(drop=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write column word to csv without header\n",
    "df[\"word\"].to_csv(\"english_words.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRE/TOEFL/IELTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "781"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRE words\n",
    "js_files = ['warm-up.js', 'intermediate.js', 'hard.js', 'high-frequency-gre.js']\n",
    "\n",
    "all_js_dfs = []\n",
    "for js_file in js_files:\n",
    "    js_df = pd.read_json(f\"data/{js_file}\")\n",
    "    all_js_dfs.append(js_df)\n",
    "\n",
    "data_1 = pd.concat(all_js_dfs, ignore_index=True)\n",
    "data_1 = data_1[\"word\"].unique()\n",
    "\n",
    "\n",
    "len(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(file_path, delimiter):\n",
    "    words = set()  # Use a set to store unique words\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            if delimiter in line:  # Only process lines containing the delimiter\n",
    "                word = line.split(delimiter)[\n",
    "                    0\n",
    "                ].strip()\n",
    "                words.add(word)  # Add the word to the set (ensures uniqueness)\n",
    "    return list(words)  # Convert the set back to a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"your_file.txt\"  # Replace with the path to your file\n",
    "delimiter = \":\"  # The delimiter can be any character, for example ':'\n",
    "unique_words = extract_words(file_path, delimiter)\n",
    "print(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
